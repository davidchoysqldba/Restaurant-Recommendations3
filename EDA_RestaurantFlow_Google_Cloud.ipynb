{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, col, desc, asc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map function for reading first record of json file with column names\n",
    "def flat_map_first_row(rec):\n",
    "    rows = []\n",
    "    json_obj = json.loads((rec))\n",
    "    for e in json_obj:\n",
    "        row_dict = {str(k): v for k, v in e.items()}\n",
    "        rows.append(Row(**row_dict))\n",
    "    return rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map function for creating rdd rows with fillin mising column\n",
    "def flat_map_json_with_fillin_missing_column(rec, column_names):\n",
    "    rows = []\n",
    "    json_obj = json.loads((rec))\n",
    "    for e in json_obj:\n",
    "        row_dict = {str(k): v for k, v in e.items()}\n",
    "        for c in column_names:\n",
    "            if c not in row_dict.keys():\n",
    "                row_dict[c] = None\n",
    "        rows.append(Row(**row_dict))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd to represent first row of all files\n",
    "rdd_first_row = sc.wholeTextFiles(data_path + '/*.json').filter(lambda x: x[1] != \"[]\\n\").map(lambda x: x[1]).map(flat_map_first_row)\n",
    "# rdd_first_row.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column name of set type that has all column names from all first json records  \n",
    "column_names = []\n",
    "\n",
    "for i in rdd_first_row.toLocalIterator():\n",
    "    column_names.extend(i.asDict().keys())\n",
    "column_names = set(column_names)\n",
    "\n",
    "# exec time < 4 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify reference to json row mapping function with column name passed in's - this will fill any missing columns with NULL \n",
    "parsing_func = functools.partial(flat_map_json_with_fillin_missing_column, column_names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read through all files with implement filling in missing column names\n",
    "rdd = sc.wholeTextFiles(data_path + '/*.json').filter(lambda x: x[1] != \"[]\\n\").map(lambda x: x[1]).flatMap(parsing_func)\n",
    "# rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pyspark dataframe from the rdd object (rdd is a row or dictionary format of data)\n",
    "df = spark.createDataFrame(rdd)\n",
    "\n",
    "# time < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[action: string, boro: string, building: string, camis: string, critical_flag: string, cuisine_description: string, dba: string, grade: string, grade_date: string, inspection_date: string, inspection_type: string, phone: string, record_date: string, score: string, street: string, violation_code: string, violation_description: string, zipcode: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 1 Define business dataframe \n",
    "#########################################################\n",
    "\n",
    "df_business = df.select([\"cuisine_description\", \"camis\", \"dba\", \"boro\", \"building\", \"phone\", \"street\", \"zipcode\", \"inspection_date\"]).distinct()#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify window function - getting last row of every camis (dobid) and latest inspection date\n",
    "\n",
    "window = Window.partitionBy(\"camis\").orderBy(desc(\"inspection_date\"))\n",
    "df_business_latest = (df_business.withColumn('rank', rank().over(window))\n",
    ".filter(col('rank') == 1)\n",
    ".drop('rank')\n",
    ")\n",
    "# df_business_latest.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(camis='50000804', score='12', grade='A', grade_date='2015-02-18T00:00:00.000'),\n",
       " Row(camis='41448559', score='9', grade='A', grade_date='2015-04-07T00:00:00.000'),\n",
       " Row(camis='41377019', score='4', grade='A', grade_date='2015-04-09T00:00:00.000'),\n",
       " Row(camis='50005214', score='9', grade='A', grade_date='2015-04-25T00:00:00.000'),\n",
       " Row(camis='41712926', score='10', grade='A', grade_date='2015-06-02T00:00:00.000')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################\n",
    "# 2 Define business grade dataframe \n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business_grade_detail = df.where(col('grade').isNotNull()).select(['camis', 'score', 'grade', 'grade_date']).distinct()\n",
    "df_business_grade_detail.take(5)\n",
    "# 4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to return grade number from letter\n",
    "\n",
    "def grade_letter_dict_to_number(key):\n",
    "    dict = {'A': 5, 'B': 4, 'C': 3}\n",
    "    return dict.get(key)\n",
    "    \n",
    "# grade_number_dict_to_letter = {'5': 'A', '4':'B' , '3': 'C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pyspark user define function for mapping grade letter to grade number\n",
    "grade_udf = F.udf(grade_letter_dict_to_number, t.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new grade number column from grade (letter) column using dict udf function\n",
    "df_business_grade_detail = df_business_grade_detail.withColumn('grade_number', grade_udf(col('grade')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude business grade d.f. of the following: exclude NULL and grade 'Not Yet Graded', 'G', 'P', 'Z'\n",
    "list_grade_exclude = {'Not Yet Graded', 'G', 'P', 'Z'}\n",
    "df_business_grade_detail2 = df_business_grade_detail.where(col('grade').isNotNull()).filter(~col('grade').isin(list_grade_exclude))#.select(col('grade')).distinct().collect()\n",
    "# df_business_grade.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business grade info d.f. of the following:\n",
    "# lowest grade, highest grade, average grade, and how many given grades\n",
    "df_business_grade = df_business_grade_detail2.groupBy(col('camis')).agg(F.min('grade_number').alias('grade_lowest'), \\\n",
    "                                            F.max('grade_number').alias('grade_highest'), \\\n",
    "                                            F.mean('grade_number').alias('grade_average'), \\\n",
    "                                            F.count('grade_number').alias('grade_count')\n",
    "                                           ) #\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(camis='50006252', grade_lowest=5, grade_highest=5, grade_average=5.0, grade_count=4),\n",
       " Row(camis='41405042', grade_lowest=5, grade_highest=5, grade_average=5.0, grade_count=3),\n",
       " Row(camis='41135090', grade_lowest=5, grade_highest=5, grade_average=5.0, grade_count=4),\n",
       " Row(camis='50003326', grade_lowest=5, grade_highest=5, grade_average=5.0, grade_count=2),\n",
       " Row(camis='41655458', grade_lowest=5, grade_highest=5, grade_average=5.0, grade_count=5)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business_grade.take(5)\n",
    "# < 1 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 3 Define business restaurant type and similarity measures dataframe \n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_similarity_score = F.udf(similarity_score, t.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(camis='40356731', cuisine_description='Ice Cream, Gelato, Yogurt, Ices', zipcode='11226'),\n",
       " Row(camis='40379662', cuisine_description='Italian', zipcode='10021'),\n",
       " Row(camis='40394329', cuisine_description='Pizza', zipcode='11234'),\n",
       " Row(camis='40512746', cuisine_description='American', zipcode='10451'),\n",
       " Row(camis='40518828', cuisine_description='CafÃ©/Coffee/Tea', zipcode='10017')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business_cuisine = df_business_latest.select(['camis', 'cuisine_description', 'zipcode']).distinct()\n",
    "df_business_cuisine.take(5)\n",
    "# < 1 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc1 = df_business_cuisine.alias('df_bc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc2 = df_business_cuisine.alias('df_bc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc_join = df_bc1.join(df_bc2, (df_bc1['zipcode'] == df_bc2['zipcode']) & (df_bc1['camis'] != df_bc2['camis']) )\n",
    "# df_bc_join.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc_joined = df_bc_join.select([\"df_bc1.camis\", \"df_bc1.cuisine_description\", \"df_bc2.camis\", \"df_bc2.cuisine_description\"]).withColumn(\"matching_camis\", col(\"df_bc2.camis\")).withColumn(\"matching_cuisine_description\", col(\"df_bc2.cuisine_description\")).drop(col(\"df_bc2.camis\")).drop(col(\"df_bc2.cuisine_description\"))#.withColumn(\"similar_score\", SequenceMatcher(None, col(\"\"))).take(2)\n",
    "# df_bc_joined.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc_compared_set = df_bc_joined.withColumn(\"similarity_score\", udf_similarity_score(col(\"cuisine_description\"), col(\"matching_cuisine_description\")))\n",
    "# df_bc_compared_set.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[camis: string, cuisine_description: string, matching_camis: string, matching_cuisine_description: string, similarity_score: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_bc_compared_set.take(20)\n",
    "df_bc_compared_set.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(camis='40786919', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='41510896', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50002099', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50056425', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50078406', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50034097', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50056428', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50007421', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='50065986', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0),\n",
       " Row(camis='40363744', cuisine_description='American', matching_camis='50034508', matching_cuisine_description='African', similarity_score=80.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bc_compared_similar_list = df_bc_compared_set.filter(col(\"similarity_score\") >= 80).orderBy(asc(\"similarity_score\"))\n",
    "df_bc_compared_similar_list.take(10)\n",
    "\n",
    "# < 2 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}